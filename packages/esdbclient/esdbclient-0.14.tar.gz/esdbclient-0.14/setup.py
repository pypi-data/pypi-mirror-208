# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['esdbclient', 'esdbclient.protos.Grpc']

package_data = \
{'': ['*']}

install_requires = \
['dnspython>=2.3.0,<3.0.0',
 'grpcio>=1.51.0,!=1.52.*',
 'protobuf>=3.11.0',
 'typing_extensions']

setup_kwargs = {
    'name': 'esdbclient',
    'version': '0.14',
    'description': 'Python gRPC Client for EventStoreDB',
    'long_description': '# Python gRPC Client for EventStoreDB\n\nThis [Python package](https://pypi.org/project/esdbclient/) provides a Python\ngRPC client for [EventStoreDB](https://www.eventstore.com/).\n\nThis client has been developed in collaboration with the EventStoreDB\nteam. Although not all the features of EventStoreDB are supported\nby this client, many of the most useful ones are presented\nin an easy-to-use interface.\n\nThis client has been tested to work with EventStoreDB LTS versions 21.10,\nwithout and without SSL/TLS, and with Python versions 3.7 to 3.11. There\nis 100% test coverage. The code has typing annotations, checked with mypy.\nThe code is formatted with black and isort, and checked with flake8. Poetry\nis used for package management during development, and for building and\npublishing distributions to [PyPI](https://pypi.org/project/esdbclient/).\n\n## Synopsis\n\nThe `ESDBClient` class can be imported from the `esdbclient` package.\n\nTo run the client, you will need a connection string URI. And, to\nconnect to a "secure" EventStoreDB server, you will also need an\nSSL/TLS certificate.\n\nProbably the three most useful methods of `ESDBClient` are:\n\n* `append_events()` This method can be used to record events in a particular\n"stream". This is useful for example when executing a command in an application\nthat mutates an aggregate. This method is "atomic" in that either all or none of\nthe events will be recorded.\n\n* `read_stream_events()` This method can be used to retrieve all the recorded\nevents in a "stream". This is useful for example when reconstructing an aggregate\nbefore executing a command in an application.\n\n* `subscribe_all_events()` This method can be used to receive all recorded\nevents across all "streams". This is useful in downstream event-processing\ncomponents, and supports processing events with "exactly-once" semantics (see below).\n\nThe example below uses an "insecure" EventStoreDB server running locally on port 2114.\n\n```python\nimport esdbclient, uuid\n\n\n# Construct ESDBClient with an EventStoreDB URI.\n\nclient = esdbclient.ESDBClient(uri="esdb://localhost:2114?Tls=false")\n\n\n# Append events to a new stream.\n\nstream_name = str(uuid.uuid4())\n\nevent1 = esdbclient.NewEvent(type=\'OrderCreated\', data=b\'data1\')\n\nclient.append_events(\n    stream_name=stream_name,\n    expected_position=None,\n    events=[event1],\n)\n\n\n# Append more events to an existing stream.\n\nevent2 = esdbclient.NewEvent(type=\'OrderUpdated\', data=b\'data2\')\nevent3 = esdbclient.NewEvent(type=\'OrderDeleted\', data=b\'data3\')\n\nclient.append_events(\n    stream_name=stream_name,\n    expected_position=0,\n    events=[event2, event3],\n)\n\n\n# Read all events recorded in a stream.\n\nrecorded = client.read_stream_events(\n    stream_name=stream_name\n)\n\nassert len(recorded) == 3\nassert recorded[0].data == event1.data\nassert recorded[1].data == event2.data\nassert recorded[2].data == event3.data\nassert recorded[0].type == event1.type\nassert recorded[1].type == event2.type\nassert recorded[2].type == event3.type\n\n\n# In an event-processing component, use a "catch-up" subscription\n# to receive all events across all streams, including events that\n# have not yet been recorded, starting from the component\'s last\n# saved "commit position".\n\nlast_saved_commit_position = 0\n\nsubscription = client.subscribe_all_events(\n    commit_position=last_saved_commit_position\n)\n\n# To implement "exactly-once" semantics, iterate over the\n# "catch-up" subscription. Process each received event,\n# in turn, through an event-processing policy. Save the\n# value of the commit_position attribute of the processed\n# event with new state generated by the policy in the same\n# atomic transaction. Use the last saved "commit position"\n# when restarting the "catch-up" subscription.\n\nreceived = []\nfor event in subscription:\n    received.append(event)\n    if event.id == event3.id:\n        break\n\nassert received[-3].data == event1.data\nassert received[-2].data == event2.data\nassert received[-1].data == event3.data\nassert received[-3].type == event1.type\nassert received[-2].type == event2.type\nassert received[-1].type == event3.type\n\nassert received[-3].commit_position > 0\nassert received[-2].commit_position > received[-3].commit_position\nassert received[-1].commit_position > received[-2].commit_position\n\n\n# Close the client after use.\n\nclient.close()\n```\n\nSee below for more details.\n\nFor an example of usage, see the [eventsourcing-eventstoredb](\nhttps://github.com/pyeventsourcing/eventsourcing-eventstoredb) package.\n\n## Table of contents\n\n<!-- TOC -->\n* [Install package](#install-package)\n  * [From PyPI](#from-pypi)\n  * [With Poetry](#with-poetry)\n* [EventStoreDB server](#eventstoredb-server)\n  * [Run container](#run-container)\n  * [Stop container](#stop-container)\n* [EventStoreDB client](#eventstoredb-client)\n  * [Import class](#import-class)\n  * [Construct client](#construct-client)\n* [Streams](#streams)\n  * [Append events](#append-events)\n  * [Append event](#append-event)\n  * [Idempotent append operations](#idempotent-append-operations)\n  * [Read stream events](#read-stream-events)\n  * [How to implement snapshotting with EventStoreDB](#how-to-implement-snapshotting-with-eventstoredb)\n  * [Read all events](#read-all-events)\n  * [Get current stream position](#get-current-stream-position)\n  * [Get current commit position](#get-current-commit-position)\n  * [Get stream metadata](#get-stream-metadata)\n  * [Set stream metadata](#set-stream-metadata)\n  * [Delete stream](#delete-stream)\n  * [Tombstone stream](#tombstone-stream)\n* [Catch-up subscriptions](#catch-up-subscriptions)\n  * [How to implement exactly-once event processing](#how-to-implement-exactly-once-event-processing)\n  * [Subscribe all events](#subscribe-all-events)\n  * [Subscribe stream events](#subscribe-stream-events)\n* [Persistent subscriptions](#persistent-subscriptions)\n  * [Create subscription](#create-subscription)\n  * [Read subscription](#read-subscription)\n  * [Get subscription info](#get-subscription-info)\n  * [List subscriptions](#list-subscriptions)\n  * [Delete subscription](#delete-subscription)\n  * [Update subscription](#update-subscription)\n  * [Create stream subscription](#create-stream-subscription)\n  * [Read stream subscription](#read-stream-subscription)\n  * [Get stream subscription info](#get-stream-subscription-info)\n  * [List stream subscriptions](#list-stream-subscriptions)\n  * [Update stream subscription](#update-stream-subscription)\n  * [Delete stream subscription](#delete-stream-subscription)\n  * [Persistent subscription consumer](#persistent-subscription-consumer)\n* [Connection](#connection)\n  * [Reconnect](#reconnect)\n  * [Close](#close)\n* [Notes](#notes)\n  * [Connection strings](#connection-strings)\n  * [Regular expression filters](#regular-expression-filters)\n  * [New event objects](#new-event-objects)\n  * [Recorded event objects](#recorded-event-objects)\n  * [Asyncio client](#asyncio-client)\n* [Contributors](#contributors)\n  * [Install Poetry](#install-poetry)\n  * [Setup for PyCharm users](#setup-for-pycharm-users)\n  * [Setup from command line](#setup-from-command-line)\n  * [Project Makefile commands](#project-makefile-commands)\n<!-- TOC -->\n\n## Install package\n\nIt is recommended to install Python packages into a Python virtual environment.\n\n### From PyPI\n\nYou can use pip to install this package directly from\n[the Python Package Index](https://pypi.org/project/esdbclient/).\n\n    $ pip install esdbclient\n\n### With Poetry\n\nYou can use Poetry to add this package to your pyproject.toml and install it.\n\n    $ poetry add esdbclient\n\n## EventStoreDB server\n\nThe EventStoreDB server can be run locally using the official Docker container image.\n\n### Run container\n\nFor development, you can run a "secure" EventStoreDB server using the following command.\n\n    $ docker run -d --name eventstoredb-secure -it -p 2113:2113 --env "HOME=/tmp" eventstore/eventstore:21.10.9-buster-slim --dev\n\nAs we will see, the client needs an EventStoreDB connection string URI as the value of\nits `uri` constructor argument. See the Notes section below for detailed information\nabout EventStoreDB connection string URIs.\n\nThe connection string for this "secure" EventStoreDB server would be:\n\n    esdb://admin:changeit@localhost:2113\n\nTo connect to a "secure" server, you will usually need to include a "username"\nand a "password" in the connection string, so that the server can authenticate the\nclient. The default username is "admin" and the default password is "changeit".\n\nWhen connecting to a "secure" server, the client also needs an SSL/TLS\ncertificate as the value of its `root_certificates` constructor argument. To connect\nto a "secure" server you will need an SSL/TLS certificate so that the client can\nauthenticate the server. For development, you can either use the SSL/TLS certificate\nof the certificate authority used to create the server\'s certificate, or when using a\nsingle-node cluster, you can use the server certificate itself. You can get the\nserver certificate with the following Python code.\n\n\n```python\nimport ssl\n\nserver_certificate = ssl.get_server_certificate(addr=(\'localhost\', 2113))\n```\n\nYou can also start an "insecure" server using the following command.\n\n    $ docker run -d --name eventstoredb-insecure -it -p 2114:2113 eventstore/eventstore:21.10.9-buster-slim --insecure\n\nThe connection string URI for this "insecure" server would be:\n\n    esdb://localhost:2114?Tls=false\n\nAs we will see, when connecting to an "insecure" server, there is no need to include\na "username" and a "password" in the connection string. If you do, these values will\nbe ignored by the client, so that they will not be sent to the server over an\ninsecure channel.\n\nPlease note, the "insecure" connection string uses a query string with the field-value\n`Tls=false`. The value of this field is by default `true`. See the Notes section below\nfor more information about EventStoreDB connection strings and the fields that can be\nused in the query string to specify connection options.\n\n### Stop container\n\nTo stop and remove the "secure" container, use the following Docker commands.\n\n    $ docker stop eventstoredb-secure\n\t$ docker rm eventstoredb-secure\n\nTo stop and remove the "insecure" container, use the following Docker commands.\n\n    $ docker stop eventstoredb-insecure\n\t$ docker rm eventstoredb-insecure\n\n\n## EventStoreDB client\n\nThis EventStoreDB client is implemented in the `esdbclient` package with\nthe `ESDBClient` class.\n\n### Import class\n\nThe `ESDBClient` class can be imported from the `esdbclient` package.\n\n```python\nfrom esdbclient import ESDBClient\n```\n\n### Construct client\n\nThe `ESDBClient` class can be constructed with a `uri` argument, which is required.\nAnd, to connect to a "secure" EventStoreDB server, the optional `root_certificates`\nargument is also required.\n\nThe `uri` argument is expected to be an EventStoreDB connection string URI that\nconforms with the standard EventStoreDB "esdb" or "esdb+discover" URI schemes. The\nsyntax and semantics of EventStoreDB connection strings are explained in the Notes\nsection below.\n\nFor example, the following connection string specifies that the client should\nattempt to create a "secure" connection to port 2113 on "localhost", and use the\nclient credentials "username" and "password" when making calls to the server.\n\n    esdb://username:password@localhost:2113?Tls=true\n\nThe client must be configured to create a "secure" connection to a "secure" server,\nor alternatively an "insecure" connection to an "insecure" server. By default, the\nclient will attempt to create a "secure" connection. And so, when connecting to an\n"insecure" server, the connection string must specify that the client should attempt\nto make an "insecure" connection.\n\nThe following connection string specifies that the client should\nattempt to create an "insecure" connection to port 2114 on "localhost".\nWhen connecting to an "insecure" server, the client will ignore any\nusername and password information included in the connection string,\nso that usernames and passwords are not sent over an "insecure" connection.\n\n    esdb://localhost:2114?Tls=false\n\nPlease note, the "insecure" connection string uses a query string with the field-value\n`Tls=false`. The value of this field is by default `true`. Unless the connection string\nURI includes the field-value `Tls=false` in the query string, the `root_certificates`\nconstructor argument is also required.\n\nWhen connecting to a "secure" server, the `root_certificates` argument is expected to\nbe a Python `str` containing PEM encoded SSL/TLS root certificates. This value is\npassed directly to `grpc.ssl_channel_credentials()`. It is used for authenticating the\nserver to the client. It is commonly the certificate of the certificate authority that\nwas responsible for generating the SSL/TLS certificate used by the EventStoreDB server.\nBut, alternatively for development, you can use the server\'s certificate itself.\n\nIn the example below, the constructor argument values are taken from the operating\nsystem environment. This is a typical arrangement in a production environment. It is\ndone this way here so that the code in this documentation can be tested with both\na "secure" and an "insecure" server.\n\n```python\nimport os\n\nclient = ESDBClient(\n    uri=os.getenv("ESDB_URI"),\n    root_certificates=os.getenv("ESDB_ROOT_CERTIFICATES"),\n)\n```\n\nSee the Notes section below for detailed information about EventStoreDB connection\nstrings and the fields that can be used in the query string to specify connection\noptions.\n\n## Streams\n\nIn EventStoreDB, a "stream" is a sequence of recorded events that all have\nthe same "stream name". There will normally be many streams in a database.\nEach recorded event has a "stream position" in its stream, and a "commit position"\nin the database. The stream positions of the recorded events in a stream is a gapless\nsequence starting from zero. The commit positions of the recorded events in the database\nform a sequence that is not gapless.\n\nThe methods `append_events()`, `read_stream_events()` and `read_all_events()` can\nbe used to record and read events in the database.\n\n### Append events\n\n*requires leader*\n\nThe `append_events()` method can be used to write a sequence of new events atomically\nto a "stream". Writing new events either creates a stream, or appends events to the end\nof a stream. This method is idempotent.\n\nThis method can be used to record atomically all the new\nevents that are generated when executing a command in an application.\n\nThree arguments are required, `stream_name`, `expected_position`\nand `events`.\n\nThe `stream_name` argument is required, and is expected to be a Python\n`str` that uniquely identifies the stream in the database.\n\nThe `expected_position` argument is required, is expected to be: `None`\nif events are being written to a new stream, and otherwise an Python `int`\nequal to the position in the stream of the last recorded event in the stream.\n\nThe `events` argument is required, and is expected to be a sequence of new\nevent objects to be appended to the named stream. The `NewEvent` class should\nbe used to construct new event objects (see the Notes section below for details\nof this class).\n\nThis method takes an optional `timeout` argument, which is a Python `float` that sets\na deadline for the completion of the gRPC operation.\n\nStreams are created by writing events. The correct value of the `expected_position`\nargument when writing the first event of a new stream is `None`. Please note, it is\nnot possible to somehow create an "empty" stream in EventStoreDB.\n\nThe stream positions of recorded events in a stream start from zero, and form a gapless\nsequence of integers. The stream position of the first recorded event in a stream is\n`0`. And so when appending the second new event to a stream that has one recorded event,\nthe correct value of the `expected_position` argument is `0`. Similarly, the stream\nposition of the second recorded event in a stream is `1`, and so when appending the\nthird new event to a stream that has two recorded events, the correct value of the\n`expected_position` argument is `1`. And so on...\n\nIf there is a mismatch between the given value of the `expected_position` argument\nand the position of the last recorded event in a stream, then an `ExpectedPositionError`\nexception will be raised. This effectively accomplishes optimistic concurrency control.\n\nIf you wish to disable optimistic concurrency control when appending new events, you\ncan set the `expected_position` to a negative integer.\n\nIf you need to discover the current position of the last recorded event in a stream,\nyou can use the `get_stream_position()` method.\n\nPlease note, the append events operation is atomic, so that either all\nor none of the given new events will be recorded. By design, it is only\npossible with EventStoreDB to atomically record new events in one stream.\n\nIn the example below, a new event is appended to a new stream.\n\n```python\nfrom uuid import uuid4\n\nfrom esdbclient import NewEvent\n\n# Construct new event object.\nevent1 = NewEvent(type=\'OrderCreated\', data=b\'data1\')\n\n# Define stream name.\nstream_name1 = str(uuid4())\n\n# Append list of events to new stream.\ncommit_position1 = client.append_events(\n    stream_name=stream_name1,\n    expected_position=None,\n    events=[event1],\n)\n```\n\nIn the example below, two subsequent events are appended to an existing\nstream.\n\n```python\nevent2 = NewEvent(type=\'OrderUpdated\', data=b\'data2\')\nevent3 = NewEvent(type=\'OrderDeleted\', data=b\'data3\')\n\ncommit_position2 = client.append_events(\n    stream_name=stream_name1,\n    expected_position=0,\n    events=[event2, event3],\n)\n```\n\nIf the operation is successful, this method returns an integer\nrepresenting the database "commit position" as it was when the operation\nwas completed.\n\nA "commit position" is a monotonically increasing integer representing\nthe position of the recorded event in a "total order" of all recorded\nevents in the database across all streams. It is the actual position\nof the event record on disk. In consequence, the sequence of commit\npositions is not gapless. Indeed, there are usually large differences\nbetween the commit positions of successive recorded events.\n\nThe "commit position" returned by `append_events()` is that of the last\nrecorded event in the given batch of new events.\n\nThe "commit position" returned in this way can therefore be used to wait\nfor a downstream component to have processed all the events that were recorded.\n\nFor example, consider a user interface command that results in the recording\nof new events, and a query into an eventually consistent materialized\nview in a downstream component that is updated from these events. If the new\nevents have not yet been processed, the view would be stale. The "commit position"\ncan be used by the user interface to poll the downstream component until it has\nprocessed those new events, after which time the view will not be stale.\n\n\n### Append event\n\n*requires leader*\n\nThe `append_event()` method can be used to write a single new event to a stream.\n\nThree arguments are required, `stream_name`, `expected_position` and `event`.\n\nThis method works in the same way as `append_events()`, however `event` is expected\nto be a single `NewEvent`.\n\nThis method takes an optional `timeout` argument, which is a Python `float` that sets\na deadline for the completion of the gRPC operation.\n\nIf the operation is successful, this method returns an integer\nrepresenting the database "commit position" as it was when the operation\nwas completed.\n\nSince the handling of a command in your application may result in one or many\nnew events, and the results of handling a command should be recorded atomically,\nand the writing of new events generated by a command handler is usually a concern\nthat is factored out and used everywhere in a project, it is more usual in a project\nto use `append_events()` to record new events. However, this method may occasionally\nbe useful, and indeed it is used by `set_stream_metadata()`.\n\n\n### Idempotent append operations\n\nSometimes it may happen that, when calling `append_events()` or `append_event()`,\nthe new events are successfully recorded, but then somehow the connection to the\ndatabase gets interrupted before the successful call can return successfully to\nthe client. In case of an error when appending an event, it is desirable to\nretry appending the same event at the same position. If the event was in fact\nsuccessfully recorded, it is convenient for the retried operation to return\nsuccessfully without raising an error due to optimistic concurrency control\n(as described above).\n\nThe example below shows the `append_events()` method being called again with\n`event3` and `expected_position=2`. We can see that repeating the call to\n`append_events()` returns successfully.\n\n```python\n# Retry appending event3.\ncommit_position_retry = client.append_events(\n    stream_name=stream_name1,\n    expected_position=0,\n    events=[event2, event3],\n)\n```\n\nWe can see that the same commit position is returned as above.\n\n```python\nassert commit_position_retry == commit_position2\n```\n\nBy calling `read_stream_events()`, we can also see the stream has been unchanged\ndespite the `append_events()` method having been called twice with the same arguments.\nThat is, there are still only three events in the stream.\n\n```python\nevents = client.read_stream_events(\n    stream_name=stream_name1\n)\n\nassert len(events) == 3\n```\n\nThis idempotent behaviour is activated because the `NewEvent` class has an `id`\nattribute that, by default, is assigned a new and unique version-4 UUID when an\ninstance of `NewEvent` is constructed. If events with the same `id` are appended\nat the same `expected_position`, the stream will be unchanged, the operation will\ncomplete successfully, and the same commit position will be returned to the caller.\n\n```python\nfrom uuid import UUID\n\n\nassert isinstance(event1.id, UUID)\nassert isinstance(event2.id, UUID)\nassert isinstance(event3.id, UUID)\n\nassert event1.id != event2.id\nassert event2.id != event3.id\n\nassert events[0].id == event1.id\nassert events[1].id == event2.id\nassert events[2].id == event3.id\n```\n\nIt is possible to set the `id` constructor argument of `NewEvent` when instantiating\nthe `NewEvent` class, but in the examples above we have been using the default\nbehaviour, which is that the `id` value is generated when the `NewEvent` class is\ninstantiated.\n\n\n### Read stream events\n\nThe `read_stream_events()` method can be used to obtain the recorded events in a\nstream. It returns a sequence of recorded events objects. The received recorded event\nobject are instances of the `RecordedEvent` class (see the Notes section below for\ndetails of this class).\n\nThis method has one required argument, `stream_name`, which is the name of\nthe stream from which to read events. By default, the recorded events in the\nstream are returned in the order they were recorded.\n\nThe method `read_stream_events()` also supports four optional arguments,\n`stream_position`, `backwards`, `limit`, and `timeout`.\n\nThe optional `stream_position` argument is an optional integer that can be used to\nindicate the position in the stream from which to start reading. This argument is\n`None` by default, which means the stream will be read either from the start of the\nstream (the default behaviour), or from the end of the stream if `backwards` is\n`True`. When reading a stream from a specific position in the stream, the\nrecorded event at that position will be included, both when reading forwards\nfrom that position, and when reading backwards from that position.\n\nThe optional `backwards` argument is a Python `bool`. The default is `False`, which\nmeans the stream will be read forwards by default, so that events are returned in the\norder they were appended, If `backwards` is `True`, the events are returned in reverse\norder.\n\nThe optional `limit` argument is an integer which restricts the number of events that\nwill be returned. The default value is `sys.maxint`.\n\nThe optional `timeout` argument is a Python `float` which sets a deadline for\nthe completion of the gRPC operation.\n\nThe example below shows how to read the recorded events of a stream\nforwards from the start of the stream to the end of the stream. The\nname of a stream is given when calling the method.\n\n```python\nevents = client.read_stream_events(\n    stream_name=stream_name1\n)\n```\n\nNow that we have a sequence of event objects, we can check we got the\nthree events that were appended to the stream, and that they are\nordered exactly as they were appended.\n\n```python\nassert len(events) == 3\n\nassert events[0].stream_name == stream_name1\nassert events[0].stream_position == 0\nassert events[0].type == event1.type\nassert events[0].data == event1.data\n\nassert events[1].stream_name == stream_name1\nassert events[1].stream_position == 1\nassert events[1].type == event2.type\nassert events[1].data == event2.data\n\nassert events[2].stream_name == stream_name1\nassert events[2].stream_position == 2\nassert events[2].type == event3.type\nassert events[2].data == event3.data\n```\n\nThe example below shows how to read recorded events in a stream forwards from\na specific stream position to the end of the stream.\n\n```python\nevents = client.read_stream_events(\n    stream_name=stream_name1,\n    stream_position=1,\n)\n\nassert len(events) == 2\n\nassert events[0].stream_name == stream_name1\nassert events[0].stream_position == 1\nassert events[0].type == event2.type\nassert events[0].data == event2.data\n\nassert events[1].stream_name == stream_name1\nassert events[1].stream_position == 2\nassert events[1].type == event3.type\nassert events[1].data == event3.data\n```\n\nThe example below shows how to read the recorded events in a stream backwards from\nthe end of the stream to the start of the stream.\n\n```python\nevents = client.read_stream_events(\n    stream_name=stream_name1,\n    backwards=True,\n)\n\nassert len(events) == 3\n\nassert events[0].stream_name == stream_name1\nassert events[0].stream_position == 2\nassert events[0].type == event3.type\nassert events[0].data == event3.data\n\nassert events[1].stream_name == stream_name1\nassert events[1].stream_position == 1\nassert events[1].type == event2.type\nassert events[1].data == event2.data\n```\n\nThe example below shows how to read a limited number (two) of the recorded events\nin a stream forwards from the start of the stream.\n\n```python\nevents = client.read_stream_events(\n    stream_name=stream_name1,\n    limit=2,\n)\n\nassert len(events) == 2\n\nassert events[0].stream_name == stream_name1\nassert events[0].stream_position == 0\nassert events[0].type == event1.type\nassert events[0].data == event1.data\n\nassert events[1].stream_name == stream_name1\nassert events[1].stream_position == 1\nassert events[1].type == event2.type\nassert events[1].data == event2.data\n```\n\nThe example below shows how to read a limited number (one) of the recorded\nevents in a stream backwards from a given stream position.\n\n```python\nevents = client.read_stream_events(\n    stream_name=stream_name1,\n    stream_position=2,\n    backwards=True,\n    limit=1,\n)\n\nassert len(events) == 1\n\nassert events[0].stream_name == stream_name1\nassert events[0].stream_position == 2\nassert events[0].type == event3.type\nassert events[0].data == event3.data\n```\n\nThe `read_stream_events()` method will raise a `NotFound` exception if the stream\ndoes not exist.\n\n```python\nfrom esdbclient.exceptions import NotFound\n\n\ntry:\n    client.read_stream_events(\'not-a-stream\')\nexcept NotFound:\n    pass  # The stream \'not-a-stream\' does not exist.\nelse:\n    raise Exception("Shouldn\'t get here")\n```\n\n### How to implement snapshotting with EventStoreDB\n\nEvent-sourced aggregates are typically reconstructed from recorded events by calling\na mutator function for each recorded event, evolving from an initial state\n`None` to the current state of the aggregate. The function `get_aggregate()` shows\nhow this can be done. The aggregate ID is used as a stream name. The exception\n`AggregateNotFound` is raised if the aggregate stream is not found.\n\n```python\ndef get_aggregate(aggregate_id, mutator_func):\n    stream_name = aggregate_id\n\n    # Get recorded events.\n    try:\n        events = client.read_stream_events(\n            stream_name=stream_name,\n            stream_position=None\n        )\n    except NotFound as e:\n        raise AggregateNotFound(aggregate_id) from e\n    else:\n        # Reconstruct aggregate from recorded events.\n        aggregate = None\n        for event in events:\n            aggregate = mutator_func(aggregate, event)\n        return aggregate\n\n\nclass AggregateNotFound(Exception):\n    """Raised when an aggregate is not found."""\n```\n\nSnapshots can improve the performance of aggregates that would otherwise be\nreconstructed from very long streams. However, it is generally recommended to\ndesign aggregates to have a finite lifecycle, and hence relatively short streams,\nthereby avoiding the need for snapshotting. This "how to" section is intended\nmerely to show how snapshotting of aggregates can be implemented with EventStoreDB\nusing this Python client.\n\nSnapshotting of aggregates can be implemented by recording the current state of\nan aggregate as a new event.\n\nIf an aggregate object has a version number that corresponds to the stream position of\nthe last event that was used to reconstruct the aggregate, and this version number\nis recorded in the snapshot metadata, then any events that are recorded after the\nsnapshot can be selected using this version number. The aggregate can then be\nreconstructed from the last snapshot and any subsequent events, without having\nto replay the entire history.\n\nWe will use a separate stream for an aggregate\'s snapshots that is named after the\nstream used for recording its events. The name of the snapshot stream will be\nconstructed by prefixing the aggregate\'s stream name with `\'$snapshot-\'`.\n\n```python\nSNAPSHOT_PREFIX = \'$snapshot-\'\n\ndef make_snapshot_stream_name(stream_name):\n    return f\'{SNAPSHOT_PREFIX}{stream_name}\'\n\n\ndef strip_snapshot_stream_name(snapshot_stream_name):\n    return snapshot_stream_name.lstrip(SNAPSHOT_PREFIX)\n```\n\nLet\'s redefine the `get_aggregate()` function, so that it looks for a snapshot event,\nthen selects subsequent aggregate events, and then calls a mutator function for each\nrecorded event. We will use JSON to serialize and deserialize Python `dict` objects.\n\nNotice that the aggregate events are read from a stream for aggregate\nevents, whilst the snapshot is read from a separate stream for aggregate snapshots.\n\n```python\nimport json\n\n\ndef get_aggregate(aggregate_id, mutator_func):\n    stream_name = aggregate_id\n    recorded_events = []\n\n    # Look for a snapshot.\n    try:\n        snapshots = client.read_stream_events(\n            stream_name=make_snapshot_stream_name(stream_name),\n            backwards=True,\n            limit=1\n        )\n    except NotFound:\n        stream_position = None\n    else:\n        snapshot = snapshots[0]\n        stream_position = deserialize(snapshot.metadata)[\'version\'] + 1\n        recorded_events.append(snapshot)\n\n    # Get subsequent events.\n    try:\n        events = client.read_stream_events(\n            stream_name=stream_name,\n            stream_position=stream_position\n        )\n    except NotFound as e:\n        raise AggregateNotFound(aggregate_id) from e\n    else:\n        recorded_events += events\n\n    # Reconstruct aggregate from recorded events.\n    aggregate = None\n    for event in recorded_events:\n        aggregate = mutator_func(aggregate, event)\n\n    return aggregate\n\n\ndef serialize(d):\n    return json.dumps(d).encode()\n\n\ndef deserialize(s):\n    return json.loads(s.decode())\n```\n\nTo show how this can be used, let\'s define an "immutable" `Dog` aggregate class, with\nattributes `name` and `tricks`. The attributes `id` and `version` will indicate an\naggregate object\'s ID and version number. The attribute `is_from_snapshot` is added\nhere merely to demonstrate below when an aggregate object has been reconstructed using\na snapshot.\n\n```python\nfrom dataclasses import dataclass\n\n\n@dataclass(frozen=True)\nclass Aggregate:\n    id: str\n    version: int\n    is_from_snapshot: bool\n\n\n@dataclass(frozen=True)\nclass Dog(Aggregate):\n    name: str\n    tricks: list\n\n```\n\nLet\'s also define a mutator function `mutate_dog()` that can evolve the state of a\n`Dog` aggregate given various different types of events, `\'DogRegistered\'`,\n`\'DogLearnedTrick\'`, and `\'$Snapshot`\'. The snapshot event type is prefixed with\n`\'$\'` so that it can be filtered out when reading all events from the database.\n\n```python\ndef mutate_dog(dog, event):\n    data = deserialize(event.data)\n    if event.type == \'DogRegistered\':\n        return Dog(\n            id=event.stream_name,\n            version=event.stream_position,\n            is_from_snapshot=False,\n            name=data[\'name\'],\n            tricks=[],\n        )\n    elif event.type == \'DogLearnedTrick\':\n        assert event.stream_position == dog.version + 1\n        assert event.stream_name == dog.id\n        return Dog(\n            id=dog.id,\n            version=event.stream_position,\n            is_from_snapshot=dog.is_from_snapshot,\n            name=dog.name,\n            tricks=dog.tricks + [data[\'trick\']],\n        )\n    elif event.type == \'$Snapshot\':\n        return Dog(\n            id=strip_snapshot_stream_name(event.stream_name),\n            version=deserialize(event.metadata)[\'version\'],\n            is_from_snapshot=True,\n            name=data[\'name\'],\n            tricks=data[\'tricks\'],\n        )\n    else:\n        raise Exception(f"Unknown event type: {event.type}")\n```\n\nFor convenience, let\'s also define a `get_dog()` function that calls `get_aggregate()`\nwith the `mutate_dog()` function as the `mutator_func` argument.\n\n```python\ndef get_dog(dog_id):\n    return get_aggregate(\n        aggregate_id=dog_id,\n        mutator_func=mutate_dog,\n    )\n```\n\nWe can also define some "command" functions that append new events to the\ndatabase. The `register_dog()` function appends a `DogRegistered` event. The\n`record_trick_learned()` appends a `DogLearnedTrick` event. The function\n`snapshot_dog()` appends a `$Snapshot` event. Notice that the\n`record_trick_learned()` and `snapshot_dog()` functions use `get_dog()`.\n\nNotice also that the `DogRegistered` and `DogLearnedTrick` events are appended to a\nstream for aggregate events, whilst the `$Snapshot` is appended to a separate stream\nfor aggregate snapshots. The snapshot event type is prefixed with `\'$\'` so that it can\nbe filtered out when reading all events from the database.\n\n```python\ndef register_dog(name):\n    dog_id = str(uuid4())\n    event = NewEvent(\n        type=\'DogRegistered\',\n        data=serialize({\'name\': name}),\n    )\n    client.append_event(\n        stream_name=dog_id,\n        expected_position=None,\n        event=event,\n    )\n    return dog_id\n\n\ndef record_trick_learned(dog_id, trick):\n    dog = get_dog(dog_id)\n    event = NewEvent(\n        type=\'DogLearnedTrick\',\n        data=serialize({\'trick\': trick}),\n    )\n    client.append_event(\n        stream_name=dog_id,\n        expected_position=dog.version,\n        event=event,\n    )\n\n\ndef snapshot_dog(dog_id):\n    dog = get_dog(dog_id)\n    event = NewEvent(\n        type=\'$Snapshot\',\n        data=serialize({\'name\': dog.name, \'tricks\': dog.tricks}),\n        metadata=serialize({\'version\': dog.version}),\n    )\n    client.append_event(\n        stream_name=make_snapshot_stream_name(dog_id),\n        expected_position=-1,\n        event=event,\n    )\n```\n\nNow we can register a new dog, and record that some tricks have been learned.\n\n```python\n# Register a new dog.\ndog_id = register_dog(\'Fido\')\n\ndog = get_dog(dog_id)\nassert dog.name == \'Fido\'\nassert dog.tricks == []\nassert dog.version == 0\nassert dog.is_from_snapshot is False\n\n\n# Record that \'Fido\' learned a new trick.\nrecord_trick_learned(dog_id, trick=\'roll over\')\n\ndog = get_dog(dog_id)\nassert dog.name == \'Fido\'\nassert dog.tricks == [\'roll over\']\nassert dog.version == 1\nassert dog.is_from_snapshot is False\n\n\n# Record that \'Fido\' learned another new trick.\nrecord_trick_learned(dog_id, trick=\'fetch ball\')\n\ndog = get_dog(dog_id)\nassert dog.name == \'Fido\'\nassert dog.tricks == [\'roll over\', \'fetch ball\']\nassert dog.version == 2\nassert dog.is_from_snapshot is False\n```\n\nAfter we call `snapshot_dog()`, the `get_dog()` function will return a `Dog`\nobject that has been constructed using the `$Snapshot` event.\n\n```python\n# Snapshot \'Fido\'.\nsnapshot_dog(dog_id)\n\ndog = get_dog(dog_id)\nassert dog.name == \'Fido\'\nassert dog.tricks == [\'roll over\', \'fetch ball\']\nassert dog.version == 2\nassert dog.is_from_snapshot is True\n```\n\nWe can continue to evolve the state of the `Dog` aggregate, using\nthe snapshot both during the call the `record_trick_learned()` and\nwhen calling `get_dog()`.\n\n```python\nrecord_trick_learned(dog_id, trick=\'sit\')\n\ndog = get_dog(dog_id)\nassert dog.name == \'Fido\'\nassert dog.tricks == [\'roll over\', \'fetch ball\', \'sit\']\nassert dog.version == 3\nassert dog.is_from_snapshot is True\n```\n\nWe can see from the `is_from_snapshot` attribute that the `Dog` object was indeed\nreconstructed from the snapshot.\n\n\n### Read all events\n\nThe method `read_all_events()` can be used to read all recorded events\nin the database in the order they were recorded. It returns an iterable of\nevents that have been recorded in the database. Iterating over this iterable\nobject will stop when it has yielded the last recorded event. The received\nrecorded event object are instances of the `RecordedEvent` class (see the\nNotes section below for details of this class).\n\nThis method supports seven optional arguments, `commit_position`, `backwards`,\n`filter_exclude`, `filter_include`, `filter_by_stream_name`, `limit`, and `timeout`.\n\nThe optional `commit_position` argument is an optional integer that can be used to\nspecify the commit position from which to start reading. This argument is `None` by\ndefault, meaning that all the events will be read either from the start, or\nfrom the end if `backwards` is `True`. Please note, if a commit position is specified,\nit must be an actually existing commit position in the database.\nAny other number will result in a server error (at least in EventStoreDB v21.10).\n\nThe optional `backwards` argument is a Python `bool`. The default is `False`, which\nmeans the database will be read forwards by default, so that events are returned in the\norder they were recorded, If `backwards` is `True`, the events are returned in reverse\norder.\n\nThe optional `filter_exclude` argument is a sequence of regular expressions that\nmatch recorded events that should not be included. This argument is ignored\nif `filter_include` is set to a non-empty sequence. By default, this argument is set\nto match the event types of "system events", so that EventStoreDB system events\nwill not normally be included. See the Notes section below for more information\nabout filter expressions.\n\nThe optional `filter_include` argument is a sequence of regular expressions\nthat match recorded events that should be included. By default, this argument\nis an empty tuple. If this argument is set to a non-empty sequence, the\n`filter_exclude` argument is ignored.\n\nThe optional `filter_by_stream_name` argument is a Python `bool` that indicates whether\nthe filter will apply to event types or stream names. By default, this value is `False`\nand so the filtering will apply to the event type strings of recorded events.\n\nThe optional `limit` argument is an integer which restricts the number of events that\nwill be returned. The default value is `sys.maxint`.\n\nThe optional `timeout` argument is a Python `float` which sets a\ndeadline for the completion of the gRPC operation.\n\nThe filtering of events is done on the EventStoreDB server. The\n`limit` argument is applied on the server after filtering.\n\nWhen reading forwards from a specific commit position, the event at the specified\nposition WILL be included. However, when reading backwards, the event at the\nspecified position will NOT be included. (This non-inclusive behaviour, of excluding\nthe specified commit position when reading all streams backwards, differs from the\nbehaviour when reading a stream backwards from a specific stream position.)\n\nThe example below shows how to read all events in the database in the\norder they were recorded.\n\n```python\nevents = list(client.read_all_events())\n\nassert len(events) >= 3\n```\n\nThe example below shows how to read all recorded events from a specific commit position.\n\n```python\nevents = list(\n    client.read_all_events(\n        commit_position=commit_position1\n    )\n)\n\nassert len(events) == 7\n\nassert events[0].stream_name == stream_name1\nassert events[0].stream_position == 0\nassert events[0].type == event1.type\nassert events[0].data == event1.data\n\nassert events[1].stream_name == stream_name1\nassert events[1].stream_position == 1\nassert events[1].type == event2.type\nassert events[1].data == event2.data\n\nassert events[2].stream_name == stream_name1\nassert events[2].stream_position == 2\nassert events[2].type == event3.type\nassert events[2].data == event3.data\n\nassert events[3].type == "DogRegistered"\nassert events[4].type == "DogLearnedTrick"\nassert events[5].type == "DogLearnedTrick"\nassert events[6].type == "DogLearnedTrick"\n```\n\nThe example below shows how to read all recorded events in reverse order.\n\n```python\nevents = list(\n    client.read_all_events(\n        backwards=True\n    )\n)\n\nassert len(events) >= 3\n\nassert events[0].type == "DogLearnedTrick"\nassert events[1].type == "DogLearnedTrick"\nassert events[2].type == "DogLearnedTrick"\nassert events[3].type == "DogRegistered"\n\nassert events[4].stream_name == stream_name1\nassert events[4].stream_position == 2\nassert events[4].type == event3.type\nassert events[4].data == event3.data\n\nassert events[5].stream_name == stream_name1\nassert events[5].stream_position == 1\nassert events[5].type == event2.type\nassert events[5].data == event2.data\n\nassert events[6].stream_name == stream_name1\nassert events[6].stream_position == 0\nassert events[6].type == event1.type\nassert events[6].data == event1.data\n\n```\n\nThe example below shows how to read a limited number (one) of the recorded events\nin the database forwards from a specific commit position. Please note, when reading\nall events forwards from a specific commit position, the event at the specified\nposition WILL be included.\n\n\n```python\nevents = list(\n    client.read_all_events(\n        commit_position=commit_position1,\n        limit=1,\n    )\n)\n\nassert len(events) == 1\n\nassert events[0].stream_name == stream_name1\nassert events[0].stream_position == 0\nassert events[0].type == event1.type\nassert events[0].data == event1.data\n\nassert events[0].commit_position == commit_position1\n```\n\nThe example below shows how to read a limited number (one) of the recorded events\nin the database backwards from the end. This gives the last recorded event.\n\n```python\nevents = list(\n    client.read_all_events(\n        backwards=True,\n        limit=1,\n    )\n)\n\nassert len(events) == 1\n\nassert events[0].type == "DogLearnedTrick"\n```\n\nThe example below shows how to read a limited number (one) of the recorded events\nin the database backwards from a specific commit position. Please note, when reading\nall events backwards from a specific commit position, the event at the specified\nposition WILL NOT be included.\n\n```python\nevents = list(\n    client.read_all_events(\n        commit_position=commit_position2,\n        backwards=True,\n        limit=1,\n    )\n)\n\nassert len(events) == 1\n\nassert events[0].commit_position < commit_position2\n```\n\n\n### Get current stream position\n\nThe `get_stream_position()` method can be used to\nget the "stream position" of the last recorded event in a stream.\n\nThis method has a `stream_name` argument, which is required.\n\nThis method also takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\nThe sequence of positions in a stream is gapless. It is zero-based,\nso that a stream with one recorded event has a current stream\nposition of `0`. The current stream position is `1` when a stream has\ntwo events, and it is `2` when there are events, and so on.\n\nIn the example below, the current stream position is obtained of the\nstream to which events were appended in the examples above.\nBecause the sequence of stream positions is zero-based, and because\nthree events were appended, so the current stream position is `2`.\n\n```python\nstream_position = client.get_stream_position(\n    stream_name=stream_name1\n)\n\nassert stream_position == 2\n```\n\nIf a stream does not exist, the returned stream position value is `None`,\nwhich matches the required expected position when appending the first event\nof a new stream (see above).\n\n```python\nstream_position = client.get_stream_position(\n    stream_name=str(uuid4())\n)\n\nassert stream_position == None\n```\n\nThis method takes an optional argument `timeout` which is a Python `float` that sets\na deadline for the completion of the gRPC operation.\n\n\n### Get current commit position\n\nThe method `get_commit_position()` can be used to get the current\ncommit position of the database.\n\n```python\ncommit_position = client.get_commit_position()\n```\n\nThis method takes an optional argument `timeout` which is a Python `float` that sets\na deadline for the completion of the gRPC operation.\n\nThis method can be useful to measure progress of a downstream component\nthat is processing all recorded events, by comparing the current commit\nposition with the recorded commit position of the last successfully processed\nevent in a downstream component.\n\nThe value of the `commit_position` argument when reading events either by using\nthe `read_all_events()` method or by using a catch-up subscription would usually\nbe determined by the recorded commit position of the last successfully processed\nevent in a downstream component.\n\n\n### Get stream metadata\n\nThe method `get_stream_metadata()` gets the metadata for a stream, along\nwith the version of the stream metadata.\n\n```python\nmetadata, metadata_version = client.get_stream_metadata(stream_name=stream_name1)\n```\n\nThe returned `metadata` value is a Python `dict`. The returned `metadata_version`\nvalue is either an `int`, or `None` if the stream does not exist. These values can\nbe passed into `set_stream_metadata()`.\n\n\n### Set stream metadata\n\n*requires leader*\n\nThe method `set_stream_metadata()` sets the metadata for a stream, along\nwith the version of the stream metadata.\n\n```python\nmetadata["foo"] = "bar"\n\nclient.set_stream_metadata(\n    stream_name=stream_name1,\n    metadata=metadata,\n    expected_position=metadata_version,\n)\n```\n\nThe `expected_position` argument should be the current version of the stream metadata.\n\nPlease refer to the EventStoreDB documentation for more information about stream\nmetadata.\n\n### Delete stream\n\n*requires leader*\n\nThe method `delete_stream()` can be used to "delete" a stream.\n\n```python\ncommit_position = client.delete_stream(stream_name=stream_name1, expected_position=2)\n```\n\nAfter deleting a stream, it\'s still possible to append new events. Reading from a\ndeleted stream will return only events that have been appended after it was\ndeleted.\n\n### Tombstone stream\n\n*requires leader*\n\nThe method `tombstone_stream()` can be used to "tombstone" a stream.\n\n```python\ncommit_position = client.tombstone_stream(stream_name=stream_name1, expected_position=2)\n```\n\nAfter tombstoning a stream, it\'s not possible to append new events.\n\n\n## Catch-up subscriptions\n\n"Catch-up" subscriptions can be used to receive events that have been recorded\nin the database, and also events that are recorded after a subscription was started.\n\nThe method `subscribe_all_events()` starts a catch-up subscription to receive all\nevents that have been and will be recorded in the database. The method\n`subscribe_stream_events()` starts a catch-up subscription to receive events from\na specific stream.\n\nCatch-up subscriptions encapsulate a streaming gRPC call which is\nkept open by the server, with newly recorded events sent to the client\nas the client iterates over the subscription response.\n\nMany catch-up subscriptions can be created, concurrently or successively, and all\nwill receive all the recorded events they have been requested to receive.\n\nThe received recorded event object are instances of the `RecordedEvent` class\n(see the Notes section below for details of this class).\n\n### How to implement exactly-once event processing\n\nThe commit positions of recorded events that are received and processed by a\ndownstream component are usefully recorded by the downstream component so that\nthe commit position of last processed event can be determined.\n\nThe last recorded commit position can be used to specify the commit position from which\nto subscribe when processing is resumed. Since this commit position will represent the\nposition of the last successfully processed event in a downstream component, so it\nwill be usual to want the next event after this position, because that is the next\nevent that has not yet been processed. For this reason, when subscribing for events\nfrom a specific commit position using a catch-up subscription in EventStoreDB, the\nrecorded event at the specified commit position will NOT be included in the sequence\nof recorded events that are received.\n\nTo accomplish "exactly-once" processing of recorded events in a downstream\ncomponent when using a catch-up subscription, the commit position of a recorded\nevent should be recorded atomically and uniquely along with the result of processing\nrecorded events, for example in the same database as materialised views when\nimplementing eventually-consistent CQRS, or in the same database as a downstream\nanalytics or reporting or archiving application. By recording the commit position\nof recorded events atomically with the new state that results from processing\nrecorded events, "dual writing" in the consumption of recorded events can be\navoided. By also recording the commit position uniquely, the new state cannot be\nrecorded twice, and hence the recorded state of the downstream component will be\nupdated only once for any recorded event. By using the greatest recorded commit\nposition to resume a catch-up subscription, all recorded events will eventually\nbe processed. The combination of the "at-most-once" condition and the "at-least-once"\ncondition gives the "exactly-once" condition.\n\nThe danger with "dual writing" in the consumption of recorded events is that if a\nrecorded event is successfully processed and new state recorded atomically in one\ntransaction with the commit position recorded in a separate transaction, one may\nhappen and not the other. If the new state is recorded but the position is lost,\nand then the processing is stopped and resumed, the recorded event may be processed\ntwice. On the other hand, if the commit position is recorded but the new state is\nlost, the recorded event may effectively not be processed at all. By either\nprocessing an event more than once, or by failing to process an event, the recorded\nstate of the downstream component might be inaccurate, or possibly inconsistent, and\nperhaps catastrophically so. Such consequences may or may not matter in your situation.\nBut sometimes inconsistencies may halt processing until the issue is resolved. You can\navoid "dual writing" in the consumption of events by atomically recording the commit\nposition of a recorded event along with the new state that results from processing that\nevent in the same atomic transaction. By making the recording of the commit positions\nunique, so that transactions will be rolled back when there is a conflict, you will\nprevent the results of any duplicate processing of a recorded event being committed.\n\nRecorded events received from a catch-up subscription cannot be acknowledged back\nto the EventStoreDB server. Acknowledging events, however, is an aspect of "persistent\nsubscriptions". Hoping to rely on acknowledging events to an upstream\ncomponent is an example of dual writing.\n\n### Subscribe all events\n\nThe`subscribe_all_events()` method can be used to start a "catch-up" subscription\nthat can return events from all streams in the database.\n\nThis method can be used by a downstream component\nto process recorded events with exactly-once semantics.\n\nThis method takes an optional `commit_position` argument, which can be\nused to specify a commit position from which to subscribe. The default\nvalue is `None`, which means the subscription will operate from the first\nrecorded event in the database. If a commit position is given, it must match\nan actually existing commit position in the database. The recoded events that\nare obtained will not include the event recorded at that commit position.\n\nThis method also takes four other optional arguments, `filter_exclude`,\n`filter_include`, `filter_by_stream_name`, and `timeout`.\n\nThe optional argument `filter_exclude` is a sequence of regular expressions that\nmatch recorded events that should not be included. This argument is ignored\nif `filter_include` is set to a non-empty sequence. By default, this argument is set\nto match the event types of "system events", so that EventStoreDB system events\nwill not normally be included. See the Notes section below for more information\nabout filter expressions.\n\nThe optional argument `filter_include` is a sequence of regular expressions\nthat match recorded events that should be included. By default, this argument\nis an empty tuple. If this argument is set to a non-empty sequence, the\n`filter_exclude` argument is ignored.\n\nThe optional argument `filter_by_stream_name` is a Python `bool` that indicates whether\nthe filter will apply to event types or stream names. By default, this value is `False`\nand so the filtering will apply to the event type strings of recorded events.\n\nPlease note, the filtering happens on the EventStoreDB server, and the\n`limit` argument is applied on the server after filtering.\n\nThe optional `timeout` argument is a Python `float` which sets a\ndeadline for the completion of the gRPC operation.\n\nThis method returns a Python iterator that yields recorded events, including events\nthat are recorded after the subscription was created. Iterating over this object will\ntherefore not stop, unless the connection to the database is lost. The call will\nbe ended when the iterator object is deleted from memory, which will happen when the\niterator object goes out of scope or is explicitly deleted. The call may also be\nclosed by the server.\n\nThe subscription object can be used directly, but it might be used within a threaded\nloop dedicated to receiving events that can be stopped in a controlled way, with\nrecorded events put on a queue for processing in a different thread. This package\ndoesn\'t provide such a threaded or queuing object class. Just make sure to reconstruct\nthe subscription (and the queue) using the last recorded commit position when resuming\nthe subscription after an error, to be sure all events are processed once.\n\nThe example below shows how to subscribe to receive all recorded\nevents from the start, and then resuming from a specific commit position.\nThree already-recorded events are received, and then three new events are\nrecorded, which are then received via the subscription.\n\n```python\n\n# Append an event to a new stream.\nstream_name2 = str(uuid4())\nevent4 = NewEvent(type=\'OrderCreated\', data=b\'data4\')\nclient.append_events(\n    stream_name=stream_name2,\n    expected_position=None,\n    events=[event4],\n)\n\n# Subscribe from the first recorded event in the database.\nsubscription = client.subscribe_all_events()\nreceived_events = []\n\n# Process events received from the catch-up subscription.\nfor event in subscription:\n    last_commit_position = event.commit_position\n    received_events.append(event)\n    if event.id == event4.id:\n        break\n\nassert received_events[-8].id == event1.id\nassert received_events[-7].id == event2.id\nassert received_events[-6].id == event3.id\nassert received_events[-5].type == "DogRegistered"\nassert received_events[-4].type == "DogLearnedTrick"\nassert received_events[-3].type == "DogLearnedTrick"\nassert received_events[-2].type == "DogLearnedTrick"\nassert received_events[-1].id == event4.id\n\n# Append subsequent events to the stream.\nevent5 = NewEvent(type=\'OrderUpdated\', data=b\'data5\')\nclient.append_events(\n    stream_name=stream_name2,\n    expected_position=0,\n    events=[event5],\n)\n\n# Receive subsequent events from the subscription.\nfor event in subscription:\n    last_commit_position = event.commit_position\n    received_events.append(event)\n    if event.id == event5.id:\n        break\n\n\n\nassert received_events[-2].id == event4.id\nassert received_events[-1].id == event5.id\n\n\n# Append more events to the stream.\nevent6 = NewEvent(type=\'OrderDeleted\', data=b\'data6\')\nclient.append_events(\n    stream_name=stream_name2,\n    expected_position=1,\n    events=[event6],\n)\n\n\n# Resume subscribing from the last commit position.\nsubscription = client.subscribe_all_events(\n    commit_position=last_commit_position\n)\n\n\n# Catch up by receiving the new event from the subscription.\nfor event in subscription:\n    received_events.append(event)\n    if event.id == event6.id:\n        break\n\nassert received_events[-3].id == event4.id\nassert received_events[-2].id == event5.id\nassert received_events[-1].id == event6.id\n\n\n# Append three more events to a new stream.\nstream_name3 = str(uuid4())\nevent7 = NewEvent(type=\'OrderCreated\', data=b\'data7\')\nevent8 = NewEvent(type=\'OrderUpdated\', data=b\'data8\')\nevent9 = NewEvent(type=\'OrderDeleted\', data=b\'data9\')\n\nclient.append_events(\n    stream_name=stream_name3,\n    expected_position=None,\n    events=[event7, event8, event9],\n)\n\n# Receive the three new events from the resumed subscription.\nfor event in subscription:\n    received_events.append(event)\n    if event.id == event9.id:\n        break\n\nassert received_events[-6].id == event4.id\nassert received_events[-5].id == event5.id\nassert received_events[-4].id == event6.id\nassert received_events[-3].id == event7.id\nassert received_events[-2].id == event8.id\nassert received_events[-1].id == event9.id\n```\n\nThe catch-up subscription call is ended as soon as the subscription object\ngoes out of scope or is explicitly deleted from memory.\n\n```python\n# End the subscription.\ndel subscription\n```\n\n### Subscribe stream events\n\nThe`subscribe_stream_events()` method can be used to start a "catch-up" subscription\nthat can return events that are recorded in a single stream.\n\nThis method takes a required `stream_name` argument, which specifies the name of the stream\nfrom which recorded events will be received.\n\nThis method also takes two optional arguments, `stream_position`, and `timeout`.\n\nThe optional `stream_position` argument specifies a position in the stream from which\nrecorded events will be received. The event at the specified stream position will not\nbe included.\n\nThe optional `timeout` argument is a Python `float` that sets\na deadline for the completion of the gRPC operation.\n\nThe example below shows how to start a catch-up subscription to a stream.\n\n```python\n\n# Subscribe to events from stream2, from the start.\nsubscription = client.subscribe_stream_events(stream_name=stream_name2)\n\n# Read from the subscription.\nevents = []\nfor event in subscription:\n    events.append(event)\n    if event.id == event6.id:\n        break\n\n# Check we got events only from stream2.\nassert len(events) == 3\nevents[0].stream_name == stream_name2\nevents[1].stream_name == stream_name2\nevents[2].stream_name == stream_name2\n\n# Append another event to stream3.\nevent10 = NewEvent(type="OrderUndeleted", data=b\'data10\')\nclient.append_events(\n    stream_name=stream_name3,\n    expected_position=2,\n    events=[event10],\n)\n\n# Append another event to stream2.\nevent11 = NewEvent(type="OrderUndeleted", data=b\'data11\')\nclient.append_events(\n    stream_name=stream_name2,\n    expected_position=2,\n    events=[event11]\n)\n\n# Continue reading from the subscription.\nfor event in subscription:\n    events.append(event)\n    if event.id == event11.id:\n        break\n\n# Check we got events only from stream2.\nassert len(events) == 4\nevents[0].stream_name == stream_name2\nevents[1].stream_name == stream_name2\nevents[2].stream_name == stream_name2\nevents[3].stream_name == stream_name2\n```\n\nThe example below shows how to start a catch-up subscription to a stream from a\nspecific stream position.\n\n```python\n\n# Subscribe to events from stream2, from the start.\nsubscription = client.subscribe_stream_events(\n    stream_name=stream_name2,\n    stream_position=1,\n)\n\n# Read event from the subscription.\nevents = []\nfor event in subscription:\n    events.append(event)\n    if event.id == event11.id:\n        break\n\n# Check we got events only after position 1.\nassert len(events) == 2\nevents[0].id == event6.id\nevents[0].stream_position == 2\nevents[0].stream_name == stream_name2\nevents[1].id == event11.id\nevents[1].stream_position == 3\nevents[1].stream_name == stream_name2\n```\n\n## Persistent subscriptions\n\n### Create subscription\n\n*requires leader*\n\nThe method `create_subscription()` can be used to create a\n"persistent subscription" to EventStoreDB.\n\nThis method takes a required `group_name` argument, which is the\nname of a "group" of consumers of the subscription.\n\nThis method also takes seven optional arguments, `from_end`, `commit_position`,\n`filter_exclude`, `filter_include`, `filter_by_stream_name`, `consumer_strategy`,\nand `timeout`.\n\nThe optional `from_end` argument can be used to specify that the group of consumers\nof the subscription should only receive events that were recorded after the subscription\nwas created.\n\nAlternatively, the optional `commit_position` argument can be used to specify a commit\nposition from which commit position the group of consumers of the subscription should\nreceive events. Please note, the recorded event at the specified commit position might\nbe included in the recorded events received by the group of consumers.\n\nIf neither `from_end` or `commit_position` are specified, the group of consumers\nof the subscription will potentially receive all recorded events in the database.\n\nThe optional `filter_exclude` argument is a sequence of regular expressions that\nmatch recorded events that should not be included. This argument is ignored\nif `filter_include` is set to a non-empty sequence. By default, this argument is set\nto match the event types of "system events", so that EventStoreDB system events\nwill not normally be received. See the Notes section below for more information\nabout filter expressions.\n\nThe optional `filter_include` argument is a sequence of regular expressions\nthat match recorded events that should be received. By default, this argument\nis an empty tuple. If this argument is set to a non-empty sequence, the\n`filter_exclude` argument is ignored.\n\nThe optional `filter_by_stream_name` argument is a Python `bool` that indicates whether\nthe filter will apply to event types or stream names. By default, this value is `False`\nand so the filtering will apply to the event type strings of recorded events.\n\nThe optional `consumer_strategy` argument is a Python `str` that defines\nthe consumer strategy for this persistent subscription. The value of this argument\ncan be `\'DispatchToSingle\'`, `\'RoundRobin\'`, `\'Pinned\'`, or `\'PinnedByCorrelation\'`. The\ndefault value is `\'DispatchToSingle\'`.\n\nThe optional `timeout` argument is a Python `float` which sets a\ndeadline for the completion of the gRPC operation.\n\nThe method `create_subscription()` does not return a value, because\nrecorded events are obtained by the group of consumers of the subscription\nusing the `read_subscription()` method.\n\nIn the example below, a persistent subscription is created.\n\n```python\n# Create a persistent subscription.\ngroup_name = f"group-{uuid4()}"\nclient.create_subscription(group_name=group_name)\n```\n\n### Read subscription\n\n*requires leader*\n\nThe method `read_subscription()` can be used by a group of consumers to receive\nrecorded events from a persistent subscription created using `create_subscription`.\n\nThis method takes a required `group_name` argument, which is\nthe name of a "group" of consumers of the subscription specified\nwhen `create_subscription()` was called.\n\nThis method also takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\nThis method returns a `PersistentSubscription` object, which is an iterator\ngiving `RecordedEvent` objects, that also has `ack()`, `nack()` and `stop()`\nmethods.\n\n```python\nsubscription = client.read_subscription(group_name=group_name)\n```\n\nThe `ack()` method should be used by a consumer to indicate to the server that it\nhas received and successfully processed a recorded event. This will prevent that\nrecorded event being received by another consumer in the same group. The `ack()`\nmethod takes an `event_id` argument, which is the ID of the recorded event that\nhas been received.\n\nThe example below iterates over the subscription object, and calls `ack()`. The\n`stop()` method is called when we have received the last event, so that we can\ncontinue with the examples below.\n\n```python\nevents = []\nfor event in subscription:\n    events.append(event)\n\n    # Acknowledge the received event.\n    subscription.ack(event_id=event.id)\n\n    # Break when the last event has been received.\n    if event.id == event11.id:\n        subscription.stop()\n```\n\nThe received events are the events we appended above.\n\n```python\nassert events[-15].id == event1.id\nassert events[-14].id == event2.id\nassert events[-13].id == event3.id\nassert events[-12].type == "DogRegistered"\nassert events[-11].type == "DogLearnedTrick"\nassert events[-10].type == "DogLearnedTrick"\nassert events[-9].type == "DogLearnedTrick"\nassert events[-8].id == event4.id\nassert events[-7].id == event5.id\nassert events[-6].id == event6.id\nassert events[-5].id == event7.id\nassert events[-4].id == event8.id\nassert events[-3].id == event9.id\nassert events[-2].id == event10.id\nassert events[-1].id == event11.id\n```\n\nThe "subscription" object also has an `nack()` method that should be used by a consumer\nto negatively acknowledge to the server that it has received but not successfully\nprocessed a recorded event. The `nack()` method takes an `event_id` argument, which is\nthe ID of the recorded event that has been received, and an `action` argument, which\nshould be a Python `str`, either `\'unknown\'`, `\'park\'`, `\'retry\'`, `\'skip\'`, or `\'stop\'`.\n\nWhilst there are some advantages of persistent subscriptions, in particular the\nprocessing of recorded events by a group of consumers, by tracking in the server\nthe position in the commit sequence of events that have been processed, there is\nthe danger of "dual writing" in the consumption of events. Reliability in processing\nof recorded events by a group of consumers will rely instead on idempotent handling\nof duplicate messages, and resilience to out-of-order delivery.\n\n### Get subscription info\n\n*requires leader*\n\nThe `get_subscription_info()` method can be used to get information for a\npersistent subscription.\n\nThis method has one required argument, `group_name`, which\nshould match the value of the argument used when calling `create_subscription()`.\n\nThis method also takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\n```python\nsubscription_info = client.get_subscription_info(\n    group_name=group_name,\n)\n```\n\nThe returned value is a `SubscriptionInfo` object.\n\n### List subscriptions\n\n*requires leader*\n\nThe `list_subscriptions()` method can be used to get information for all\nexisting persistent subscriptions.\n\nThis method takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\n```python\nsubscriptions = client.list_subscriptions()\n```\n\nThe returned value is a list of `SubscriptionInfo` objects.\n\n### Update subscription\n\n*requires leader*\n\nThe method `update_subscription()` can be used to update a\n"persistent subscription".\n\nThis method takes a required `group_name` argument, which is the\nname of a "group" of consumers of the subscription.\n\nThis method also takes three optional arguments, `from_end`, `commit_position`,\nand `timeout`.\n\nThe optional `from_end` argument can be used to specify that the group of consumers\nof the subscription should only receive events that were recorded after the subscription\nwas updated.\n\nAlternatively, the optional `commit_position` argument can be used to specify a commit\nposition from which commit position the group of consumers of the subscription should\nreceive events. Please note, the recorded event at the specified commit position might\nbe included in the recorded events received by the group of consumers.\n\nIf neither `from_end` or `commit_position` are specified, the group of consumers\nof the subscription will potentially receive all recorded events in the database.\n\nPlease note, the filter options and consumer strategy cannot be adjusted.\n\nThe optional `timeout` argument is a Python `float` which sets a\ndeadline for the completion of the gRPC operation.\n\nThe method `update_subscription()` does not return a value.\n\nIn the example below, a persistent subscription is updated to run from the end of the\ndatabase.\n\n```python\n# Create a persistent subscription.\nclient.update_subscription(group_name=group_name, from_end=True)\n```\n\n### Delete subscription\n\n*requires leader*\n\nThe `delete_subscription()` method can be used to delete a persistent\nsubscription.\n\nThis method has one required argument, `group_name`, which\nshould match the value of argument used when calling `create_subscription()`.\n\nThis method also takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\n```python\nclient.delete_subscription(\n    group_name=group_name,\n)\n```\n\n### Create stream subscription\n\n*requires leader*\n\nThe `create_stream_subscription()` method can be used to create a persistent\nsubscription for a stream.\n\nThis method has two required arguments, `group_name` and `stream_name`. The\n`group_name` argument names the group of consumers that will receive events\nfrom this subscription. The `stream_name` argument specifies which stream\nthe subscription will follow. The values of both these arguments are expected\nto be Python `str` objects.\n\nThe method also takes four optional arguments, `stream_position`, `from_end`,\n`consumer_strategy`, and `timeout`.\n\nThis optional `stream_position` argument specifies a stream position from\nwhich to subscribe. The recorded event at this stream\nposition will be received when reading the subscription.\n\nThis optional `from_end` argument is a Python `bool`.\nBy default, the value of this argument is `False`. If this argument is set\nto `True`, reading from the subscription will receive only events\nrecorded after the subscription was created. That is, it is not inclusive\nof the current stream position.\n\nThe optional `consumer_strategy` argument is a Python `str` that defines\nthe consumer strategy for this persistent subscription. The value of this argument\ncan be `\'DispatchToSingle\'`, `\'RoundRobin\'`, `\'Pinned\'`, or `\'PinnedByCorrelation\'`. The\ndefault value is `\'DispatchToSingle\'`.\n\nThis method also takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\nThis method does not return a value. Events can be received by iterating\nover the value returned by calling `read_stream_subscription()`.\n\nThe example below creates a persistent stream subscription from the start of the stream.\n\n```python\n# Create a persistent stream subscription from start of the stream.\ngroup_name1 = f"group-{uuid4()}"\nclient.create_stream_subscription(\n    group_name=group_name1,\n    stream_name=stream_name2,\n)\n```\n\nThe example below creates a persistent stream subscription from a stream position.\n\n```python\n# Create a persistent stream subscription from a stream position.\ngroup_name2 = f"group-{uuid4()}"\nclient.create_stream_subscription(\n    group_name=group_name2,\n    stream_name=stream_name2,\n    stream_position=2\n)\n```\n\nThe example below creates a persistent stream subscription from the end of the stream.\n\n```python\n# Create a persistent stream subscription from end of the stream.\ngroup_name3 = f"group-{uuid4()}"\nclient.create_stream_subscription(\n    group_name=group_name3,\n    stream_name=stream_name2,\n    from_end=True\n)\n```\n\n### Read stream subscription\n\n*requires leader*\n\nThe `read_stream_subscription()` method can be used to read a persistent\nstream subscription.\n\nThis method has two required arguments, `group_name` and `stream_name`, which\nshould match the values of arguments used when calling `create_stream_subscription()`.\n\nThis method also takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\nThis method returns a `PersistentSubscription` object, which is an iterator\ngiving `RecordedEvent` objects, that also has `ack()`, `nack()` and `stop()`\nmethods.\n\n```python\nsubscription = client.read_stream_subscription(\n    group_name=group_name1,\n    stream_name=stream_name2,\n)\n```\n\nThe example below iterates over the subscription object, and calls `ack()`.\nThe for loop breaks when we have received the last event in the stream, so\nthat we can finish the examples in this documentation.\n\n```python\nevents = []\nfor event in subscription:\n    events.append(event)\n\n    # Acknowledge the received event.\n    subscription.ack(event_id=event.id)\n\n    # Stop when \'event11\' has been received.\n    if event.id == event11.id:\n        subscription.stop()\n```\n\nWe can check we received all the events that were appended to `stream_name2`\nin the examples above.\n\n```python\nassert len(events) == 4\nassert events[0].stream_name == stream_name2\nassert events[0].id == event4.id\nassert events[1].stream_name == stream_name2\nassert events[1].id == event5.id\nassert events[2].stream_name == stream_name2\nassert events[2].id == event6.id\nassert events[3].stream_name == stream_name2\nassert events[3].id == event11.id\n```\n\n### Get stream subscription info\n\n*requires leader*\n\nThe `get_stream_subscription_info()` method can be used to get information for a\npersistent subscription for a stream.\n\nThis method has two required arguments, `group_name` and `stream_name`, which\nshould match the values of arguments used when calling `create_stream_subscription()`.\n\nThis method also takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\n```python\nsubscription_info = client.get_stream_subscription_info(\n    group_name=group_name1,\n    stream_name=stream_name2,\n)\n```\n\nThe returned value is a `SubscriptionInfo` object.\n\n### List stream subscriptions\n\n*requires leader*\n\nThe `list_stream_subscriptions()` method can be used to get information for all\nthe persistent subscriptions for a stream.\n\nThis method has one required argument, `stream_name`.\n\nThis method also takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\n```python\nsubscriptions = client.list_stream_subscriptions(\n    stream_name=stream_name2,\n)\n```\n\nThe returned value is a list of `SubscriptionInfo` objects.\n\n### Update stream subscription\n\n*requires leader*\n\nThe method `update_stream_subscription()` can be used to update a\npersistent subscription for a stream.\n\nThis method takes a required `group_name` argument, which is the\nname of a "group" of consumers of the subscription, and a required\n`stream_name` argument, which is the name of a stream.\n\nThis method also takes three optional arguments, `from_end`, `stream_position`,\nand `timeout`.\n\nThe optional `from_end` argument can be used to specify that the group of consumers\nof the subscription should only receive events that were recorded after the subscription\nwas updated.\n\nAlternatively, the optional `stream_position` argument can be used to specify a stream\nposition from which commit position the group of consumers of the subscription should\nreceive events. Please note, the recorded event at the specified stream position might\nbe included in the recorded events received by the group of consumers.\n\nIf neither `from_end` or `commit_position` are specified, the group of consumers\nof the subscription will potentially receive all recorded events in the stream.\n\nPlease note, the consumer strategy cannot be adjusted.\n\nThe optional `timeout` argument is a Python `float` which sets a\ndeadline for the completion of the gRPC operation.\n\nThe method `update_stream_subscription()` does not return a value.\n\nIn the example below, a persistent subscription for a stream is updated to run from the\nend of the stream.\n\n```python\n# Create a persistent subscription.\nclient.update_stream_subscription(\n    group_name=group_name1,\n    stream_name=stream_name2,\n    from_end=True,\n)\n```\n\n### Delete stream subscription\n\n*requires leader*\n\nThe `delete_stream_subscription()` method can be used to delete a persistent\nsubscription for a stream.\n\nThis method has two required arguments, `group_name` and `stream_name`, which\nshould match the values of arguments used when calling `create_stream_subscription()`.\n\nThis method also takes an optional `timeout` argument, that\nis expected to be a Python `float`, which sets a deadline\nfor the completion of the gRPC operation.\n\n```python\nclient.delete_stream_subscription(\n    group_name=group_name1,\n    stream_name=stream_name2,\n)\n```\n\n### Persistent subscription consumer\n\nThe reading of a persistent subscription can be encapsulated in a "consumer" that calls\na "policy" function when a recorded event is received and then automatically calls\n`ack()` if the policy function returns normally, and `nack()` if it raises an exception,\nperhaps retrying the event for a certain number of times, and then parking the event.\n\nThe simple example below shows how this might be done. We can see that \'event11\' is\nacknowledged before \'event5\' is finally parked.\n\n\n```python\nacked_events = {}\nnacked_events = {}\n\n\nclass ExampleConsumer:\n    def __init__(self, subscription, max_retries, final_action):\n        self.subscription = subscription\n        self.max_retries = max_retries\n        self.final_action = final_action\n        self.error = None\n\n    def run(self):\n        try:\n            for event in self.subscription:\n                try:\n                    self.policy(event)\n                except Exception:\n                    if event.retry_count < self.max_retries:\n                        action = "retry"\n                    else:\n                        action = self.final_action\n                    self.subscription.nack(event.id, action=action)\n                    self.after_nack(event, action)\n                else:\n                    self.subscription.ack(event.id)\n                    self.after_ack(event)\n        except Exception:\n            self.subscription.stop()\n            raise\n\n    def stop(self):\n        self.subscription.stop()\n\n    def policy(self, event):\n        # Raise an exception when we see "event5".\n        if event.id == event5.id:\n            raise Exception()\n\n    def after_ack(self, event):\n        # Track retry count of acked events.\n        acked_events[event.id] = event.retry_count\n\n    def after_nack(self, event, action):\n        # Track retry count of nacked events.\n        nacked_events[event.id] = event.retry_count\n\n        if action == self.final_action:\n            # Stop the consumer, so we can continue with the examples.\n            self.stop()\n\n\n# Create subscription.\ngroup_name = f"group-{uuid4()}"\nclient.create_subscription(group_name, commit_position=commit_position1)\n\n# Read subscription.\nsubscription = client.read_subscription(group_name)\n\n# Construct consumer.\nconsumer = ExampleConsumer(\n    subscription=subscription,\n    max_retries=5,\n    final_action="park",\n)\n\n# Run consumer.\nconsumer.run()\n\n# Check \'event11\' was acked and never retried.\nassert acked_events[event11.id] == 0\nassert event11.id not in nacked_events\n\n# Check \'event5\' was retried five times and never acked.\nassert nacked_events[event5.id] == 5\nassert event5.id not in acked_events\n```\n\n## Connection\n\n### Reconnect\n\nThe `reconnect()` method can be used to manually reconnect the client to a\nsuitable EventStoreDB node. This method uses the same routine for reading the\ncluster node states and then connecting to a suitable node according to the\nclient\'s node preference that is specified in the connection string URI when\nthe client is constructed. This method is thread-safe, and it is "conservative"\nin that, when it is called by several threads at the same time, only one\nreconnection will occur. Concurrent attempts to reconnect will block until\nthe client has reconnected successfully, and then they will all return normally.\n\n```python\nclient.reconnect()\n```\n\nAn example of when it might be desirable to reconnect manually is when (for performance\nreasons) the client\'s node preference is to be connected to a follower node in the\ncluster, and, after a cluster leader election, the follower becomes the leader.\nReconnecting to a follower node in this case is currently beyond the capabilities of\nthis client, but this behavior might be implemented in a future release.\n\nPlease note, nearly all the client methods are decorated with `@autoreconnect` (which\ncalls the `reconnect()` method when the client detects that reconnecting is required)\nand a `@retry` decorator that more generally will retry operations that fail due to\nconnection issues.\n\nThe `@autoreconnect` decorator will reconnect to a suitable node in the cluster when\nthe server to which the client has been connected has become unavailable, or when the\nclient\'s gRPC channel happens to have been closed. The client will also reconnect when\na method is called that requires a leader, and the client\'s node preference is to be\nconnected to a leader, but the node that the client has been connected to stops being\nthe leader. In this case, the client will reconnect to the current leader. After\nreconnecting, the failed operation will be retried.\n\nThe `@retry` decorator retries operations that have failed due to more general\nconnection issues, such as a deadline being reached (so that the operation times\nout), or in case the server throws an exception when handling a client request.\n\nPlease also note, the aspects not covered by the reconnect and retry decorator\nbehaviours have to do with methods that return iterators. For example, consider\nthe `read_all_events()` method, which returns an iterator of `RecordedEvent` objects.\nThe method has returned and the method decorators have therefore exited before the\niterating of the response begins. Therefore, it isn\'t possible for this method to\ntrigger a reconnection or for it to retry, when the streaming response somehow fails.\n\nAnother example is the "catch-up" and persistent subscription read methods, although\nwith these methods there is an initial "confirmation" response from the server which\nis received and checked by the client before the method returns, so if the server is\nunavailable when the call is made, or if the channel is somehow closed, or if the\nserver throws an error for some reason, then the client will either reconnect and\nretry, or more simply just retry the operation, according the type of error that\nwas encountered. However, an event-processing component that iterates over a\nsuccessfully returned "catch-up" subscription response will need to be monitored for\nerrors, and, if it fails after it has started iterating over the response, the catch-up\nsubscription will need to be restarted from the event-processing component\'s "last\nsaved commit position". If the event-processing component is recording the commit\nposition of each recorded event atomically along with any new state that results from\nprocessing the recorded events, and the commit positions are recorded with a uniqueness\nconstraint, then there will no danger to the projected state becoming inconsistent due\nto these connection issues. In this case, the client will automatically reconnect to a\nnode in the cluster when the subsequent call to a catch-up subscription method is made.\nYou just need to handle the exception that occurs from the iterator, then read your last\nsaved commit position, and then restart your event-processing component, using the same\nclient object. The client will reconnect if there is a need to do so when the subsequent\ncall is made. Similarly, when reading persistent subscriptions, if there are\nconnectivity issues after you have started iterating over a successfully received\nresponse, then the server call will need to be restarted. In this case, the received\nevents will start with the last saved checkpoint on the server (potentially returning\nevents that were in fact successfully processed).\n\n\n### Close\n\nThe `close()` method can be used to cleanly close the gRPC connection.\n\n```python\nclient.close()\n```\n\n## Notes\n\n### Connection strings\n\nThe EventStoreDB connection string is a URI that conforms with one of two possible\nschemes, either the "esdb" scheme or the "esdb+discover" scheme. The syntax and\nsemantics of the EventStoreDB URI schemes are explained below. The syntax is\ndefined using [EBNF](https://en.wikipedia.org/wiki/Extended_BackusNaur_form).\n\nThe "esdb" URI scheme can be defined in the following way.\n\n    esdb-uri = "esdb://" , [ user-info , "@" ] , grpc-target, { "," , grpc-target } , [ "?" , query-string ] ;\n\nIn the "esdb" URI scheme, after the optional user info string, there must be at least\none gRPC target. If there are several gRPC targets, they must be separated from each\nother with the "," character. Each gRPC target should indicate an EventStoreDB gRPC\nserver socket, by specifying a host and a port number separated with the ":" character.\nThe host may be a hostname that can be resolved to an IP address, or an IP address.\n\n    grpc-target = ( hostname | ip-address ) , ":" , port-number ;\n\n\nThe "esdb+discover" URI scheme can be defined in the following way.\n\n    esdb-discover-uri = "esdb+discover://" , [ user-info, "@" ] , cluster-domainname , [ "?" , query-string ] ;\n\nIn the "esdb+discover" URI scheme, after the user info string, there must be a domain\nname which should identify a cluster of EventStoreDB servers. The client will use a DNS\nserver to resolve the domain name to a list of addresses of EventStoreDB servers,\nby querying for \'A\' records. In this case, the port number "2113" will be used to\nconstruct gRPC targets from the addresses obtained from \'A\' records provided by the\nDNS server. Therefore, if you want to use the "esdb+discover" URI scheme, you will\nneed to configure DNS when setting up your EventStoreDB cluster.\n\nWith both the "esdb" and "esdb+disocver" URI schemes, the client firstly obtains\na list of gRPC targets: either directly from "esdb" connection strings; or indirectly\nfrom "esdb+discover" connection strings via DNS. This list of targets is known as the\n"gossip seed". The client will then attempt to connect to each gRPC target in turn,\nattempting to call the EventStoreDB Gossip API to obtain information about the\nEventStoreDB cluster. A member of the cluster is selected by the client, according\nto the "node preference" option. The client may then need to close its\nconnection and reconnect to the selected server.\n\nIn both the "esdb" and "esdb+discover" schemes, the URI may include a user info string.\nIf it exists in the URI, the user info string must be separated from the rest of the URI\nwith the "@" character. The user info string must include a username and a password,\nseparated with the ":" character.\n\n    user-info = username , ":" , password ;\n\nThe user info is sent by the client as "call credentials" in each call to a "secure"\nserver, in a "basic auth" authorization header. This authorization header is used by\nthe server to authenticate the client. The authorization header is not sent to\n"insecure" servers.\n\nIn both the "esdb" and "esdb+discover" schemes, the optional query string must be one\nor many field-value arguments, separated from each other with the "&" character.\n\n    query-string = field-value, { "&", field-value } ;\n\nEach field-value argument must be one of the supported fields, and an\nappropriate value, separated with the "=" character.\n\n    field-value = ( "Tls", "=" , "true" | "false" )\n                | ( "TlsVerifyCert", "=" , "true" | "false" )\n                | ( "ConnectionName", "=" , string )\n                | ( "NodePreference", "=" , "leader" | "follower" | "readonlyreplica" | "random" )\n                | ( "DefaultDeadline", "=" , integer )\n                | ( "GossipTimeout", "=" , integer )\n                | ( "MaxDiscoverAttempts", "=" , integer )\n                | ( "DiscoveryInterval", "=" , integer )\n                | ( "MaxDiscoverAttempts", "=" , integer )\n                | ( "KeepAliveInterval", "=" , integer )\n                | ( "KeepAliveInterval", "=" , integer ) ;\n\nThe table below describes the query field-values supported by this client.\n\n| Field               | Value                                                                 | Description                                                                                                                                                       |\n|---------------------|-----------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Tls                 | "true", "false" (default: "true")                                     | If "true" the client will create a "secure" gRPC channel. If "false" the client will create an "insecure" gRPC channel. This must match the server configuration. |\n| TlsVerifyCert       | "true", "false" (default: "true")                                     | This value is currently ignored.                                                                                                                                  |\n| ConnectionName      | string (default: auto-generated version-4 UUID)                       | Sent in call metadata for every call, to identify the client to the cluster.                                                                                      |\n| NodePreference      | "leader", "follower", "readonlyreplica", "random" (default: "leader") | The node state preferred by the client. The client will select a node from the cluster info received from the Gossip API according to this preference.            |\n| DefaultDeadline     | integer (default: `None`)                                             | The default value (in seconds) of the `timeout` argument of client "write" methods such as `append_events()`.                                                     |\n| GossipTimeout       | integer (default: 5)                                                  | The default value (in seconds) of the `timeout` argument of gossip read methods, such as `read_gossip()`.                                                         |\n| MaxDiscoverAttempts | integer (default: 10)                                                 | The number of attempts to read gossip when connecting or reconnecting to a cluster member.                                                                        |\n| DiscoveryInterval   | integer (default: 100)                                                | How long to wait (in milliseconds) between gossip retries.                                                                                                        |\n| KeepAliveInterval   | integer (default: `None`)                                             | The value of the "grpc.keepalive_ms" gRPC channel option.                                                                                                         |\n| KeepAliveTimeout    | integer (default: `None`)                                             | The value of the "grpc.keepalive_timeout_ms" gRPC channel option.                                                                                                 |\n\n\nHere are some examples of EventStoreDB connection string URIs.\n\n    # Get cluster info from secure server socket localhost:2113,\n    # and use "admin" and "changeit" as username and password\n    # when making calls to EventStoreDB API methods.\n\n    esdb://admin:changeit@localhost:2113\n\n\n    # Get cluster info from insecure server socket 127.0.0.1:2114\n\n    esdb://127.0.0.1:2114?Tls=false\n\n\n    # Get cluster info from addresses in \'A\' records for cluster1.example.com,\n    # and use a default deadline for making calls to EventStore API method.\n\n    esdb+discover://admin:changeit@cluster1.example.com?DefaultDeadline=5\n\n\n    # Get cluster info from either localhost:2111 or localhost:2112 or\n    # localhost:2113, and then connect to a follower node in the cluster.\n\n    esdb://admin:changeit@localhost:2111,localhost:2112,localhost:2113?NodePreference=follower\n\n\n    # Get cluster info from addresses in \'A\' records for cluster1.example.com,\n    # and configure "keep alive" timeout and interval in the gRPC channel.\n\n    esdb+discover://admin:changeit@cluster1.example.com?KeepAliveInterval=10000&KeepAliveTimeout=10000\n\n\nPlease note, the client is insensitive to the case of fields and values. If fields are\nrepeated in the query string, the query string will be parsed without error. However,\nthe connection options used by the client will use the value of the first field. All\nthe other field-values in the query string with the same field name will be ignored.\nFields without values will also be ignored.\n\nIf the client\'s node preference is "leader" and the node becomes a\n"follower", the client will attempt to reconnect to the current leader when a method\nis called that expects to call a leader. Methods which mutate the state of the database\nexpect to call a leader. For such methods, the HTTP header"requires-leader" is set to\n"true", and this header is observed by the server, and so a node which is not a leader\nthat receives such a request will return an error. This error is detected by the client,\nwhich will then close the current gRPC connection and create a new connection to the\nleader. The request will then be retried with the leader.\n\nIf the client\'s node preference is "follower" and there are no follower\nnodes in the cluster, then the client will raise an exception. Similarly, if the\nclient\'s node preference is "readonlyreplica" and there are no read-only replica\nnodes in the cluster, then the client will also raise an exception.\n\nThe gRPC channel option "grpc.max_receive_message_length" is automatically\nconfigured to the value `17 * 1024 * 1024`. This value cannot be changed.\n\n\n### Regular expression filters\n\nThe filter arguments in `read_all_events()`, `subscribe_all_events()`,\n`create_subscription()` and `get_commit_position()` are applied to the `type`\nattribute of recorded events.\n\nThe default value of the `filter_exclude` arguments is designed to exclude\nEventStoreDB "system" and "persistence subscription config" events, which\notherwise would be included. System events generated by EventStoreDB all\nhave `type` strings that start with the `$` sign. Persistence subscription\nevents generated when manipulating persistence subscriptions all have `type`\nstrings that start with `PersistentConfig`.\n\nFor example, to match the type of EventStoreDB system events, use the regular\nexpression `r\'\\$.+\'`. Please note, the constant `ESDB_SYSTEM_EVENTS_REGEX` is\nset to `r\'\\$.+\'`. You can import this value\n(`from esdbclient import ESDB_SYSTEM_EVENTS_REGEX`) and use\nit when building longer sequences of regular expressions.\n\nSimilarly, to match the type of EventStoreDB persistence subscription events, use the\nregular expression `r\'PersistentConfig\\d+\'`. The constant `ESDB_PERSISTENT_CONFIG_EVENTS_REGEX`\nis set to `r\'PersistentConfig\\d+\'`. You can also import this value\n(`from esdbclient import ESDB_PERSISTENT_CONFIG_EVENTS_REGEX`) and use it when building\nlonger sequences of regular expressions.\n\nThe constant `DEFAULT_EXCLUDE_FILTER` is a sequence of regular expressions that match\nthe events that EventStoreDB generates internally, events that are extraneous to those\nwhich you append using the `append_events()` method.\n\n\n### New event objects\n\nThe `NewEvent` class is used when appending events.\n\nThe required argument `type` is a Python `str`, used to indicate the type of\nthe event that will be recorded.\n\nThe required argument `data` is a Python `bytes` object, used to indicate the data of\nthe event that will be recorded.\n\nThe optional argument `metadata` is a Python `bytes` object, used to indicate any\nmetadata of the event that will be recorded. The default value is an empty `bytes`\nobject.\n\nThe optional argument `content_type` is a Python `str`, used to indicate the\ntype of the data that will be recorded for this event. The default value is\n`application/json`, which indicates that the `data` was serialised using JSON.\nAn alternative value for this argument is `application/octet-stream`.\n\nThe optional argument `id` is a Python `UUID` object, used to specify the unique ID\nof the event that will be recorded. This value will default to a new version-4 UUID.\n\n```python\nnew_event1 = NewEvent(\n    type=\'OrderCreated\',\n    data=b\'{"name": "Greg"}\',\n)\nassert new_event1.type == \'OrderCreated\'\nassert new_event1.data == b\'{"name": "Greg"}\'\nassert new_event1.metadata == b\'\'\nassert new_event1.content_type == \'application/json\'\nassert isinstance(new_event1.id, UUID)\n\nevent_id = uuid4()\nnew_event2 = NewEvent(\n    type=\'ImageCreated\',\n    data=b\'01010101010101\',\n    metadata=b\'{"a": 1}\',\n    content_type=\'application/octet-stream\',\n    id=event_id,\n)\nassert new_event2.type == \'ImageCreated\'\nassert new_event2.data == b\'01010101010101\'\nassert new_event2.metadata == b\'{"a": 1}\'\nassert new_event2.content_type == \'application/octet-stream\'\nassert new_event2.id == event_id\n```\n\n### Recorded event objects\n\nThe `RecordedEvent` class is used when reading events.\n\nThe attribute `type` is a Python `str`, used to indicate the type of event\nthat was recorded.\n\nThe attribute `data` is a Python `bytes` object, used to indicate the data of the\nevent that was recorded.\n\nThe attribute `metadata` is a Python `bytes` object, used to indicate the metadata of\nthe event that was recorded.\n\nThe attribute `content_type` is a Python `str`, used to indicate the type of\ndata that was recorded for this event (usually `application/json` to indicate that\nthis data can be parsed as JSON, but alternatively `application/octet-stream` to\nindicate that it is something else).\n\nThe attribute `id` is a Python `UUID` object, used to indicate the unique ID of the\nevent that was recorded. Please note, when recorded events are returned from a call\nto `read_stream_events()` in EventStoreDB v21.10, the commit position is not actually\nset in the response. This attribute is typed as an optional value (`Optional[UUID]`),\nand in the case of using EventStoreDB v21.10 the value of this attribute will be `None`\nwhen reading recorded events from a stream. Recorded events will however have this\nvalues set when reading recorded events from `read_all_events()` and from both\ncatch-up and persistent subscriptions.\n\nThe attribute `stream_name` is a Python `str`, used to indicate the name of the\nstream in which the event was recorded.\n\nThe attribute `stream_position` is a Python `int`, used to indicate the position in the\nstream at which the event was recorded.\n\nThe attribute `commit_position` is a Python `int`, used to indicate the commit position\nat which the event was recorded.\n\n```python\nfrom esdbclient.events import RecordedEvent\n\nrecorded_event = RecordedEvent(\n    type=\'OrderCreated\',\n    data=b\'{}\',\n    metadata=b\'\',\n    content_type=\'application/json\',\n    id=uuid4(),\n    stream_name=\'stream1\',\n    stream_position=0,\n    commit_position=512,\n)\n```\n\n### Asyncio client\n\nThe `esdbclient` package also includes an early version of an asynchronous I/O\ngRPC Python client. It follows exactly the same behaviors as the multithreaded\n`ESDBClient`, but uses the `grpc.aio` package and the `asyncio` module, instead of\n`grpc` and `threading`.\n\nThe `async` function `AsyncioESDBClient` constructs the client, and connects to\na server. It can be imported from `esdbclient`, and can be called with the same\narguments as `ESDBClient`. It supports both the "esdb" and the "esdb+discover"\nconnection string URI schemes, and reconnects and retries methods when connection\nissues are encountered, just like `ESDBClient`.\n\n```python\nfrom esdbclient import AsyncioESDBClient\n```\n\nThe asynchronous I/O client has `async` methods `append_events()`,\n`read_stream_events()`, `read_all_events()`, `subscribe_all_events()`,\n`delete_stream()`, `tombstone_stream()`, and `reconnect()`.\n\nThese methods are equivalent to the methods on `ESDBClient`. They have the same\nmethod signatures, and can be called with the same arguments, to the same effect.\nThe methods which appear on `ESDBClient` but not on `AsyncioESDBClient` will be\nadded soon.\n\nThe example below demonstrates the `append_events()`, `read_stream_events()` and\n`subscribe_all_events()` methods. These are the most useful methods for writing\nan event-sourced application, allowing new aggregate events to be recorded, the\nrecorded events of an aggregate to be obtained so aggregates can be reconstructed,\nand the state of an application to propagated and processed with "exactly-once"\nsemantics.\n\n```python\nimport asyncio\n\nasync def demonstrate_asyncio_client():\n\n    # Construct client.\n    client = await AsyncioESDBClient(\n        uri=os.getenv("ESDB_URI"),\n        root_certificates=os.getenv("ESDB_ROOT_CERTIFICATES"),\n    )\n\n    # Append events.\n    stream_name = str(uuid4())\n    event1 = NewEvent("OrderCreated", data=b\'{}\')\n    event2 = NewEvent("OrderUpdated", data=b\'{}\')\n    event3 = NewEvent("OrderDeleted", data=b\'{}\')\n\n    await client.append_events(\n        stream_name=stream_name,\n        expected_position=None,\n        events=[event1, event2, event3]\n    )\n\n    # Read stream events.\n    recorded = await client.read_stream_events(stream_name)\n    assert len(recorded) == 3\n    assert recorded[0].id == event1.id\n    assert recorded[1].id == event2.id\n    assert recorded[2].id == event3.id\n\n\n    # Subscribe all events.\n    received = []\n    async for event in await client.subscribe_all_events():\n        received.append(event)\n        if event.id == event3.id:\n            break\n    assert received[-3].id == event1.id\n    assert received[-2].id == event2.id\n    assert received[-1].id == event3.id\n\n\n    # Close the client.\n    await client.close()\n\n\n# Run the demo.\nasyncio.get_event_loop().run_until_complete(\n    demonstrate_asyncio_client()\n)\n```\n\n## Contributors\n\n### Install Poetry\n\nThe first thing is to check you have Poetry installed.\n\n    $ poetry --version\n\nIf you don\'t, then please [install Poetry](https://python-poetry.org/docs/#installing-with-the-official-installer).\n\n    $ curl -sSL https://install.python-poetry.org | python3 -\n\nIt will help to make sure Poetry\'s bin directory is in your `PATH` environment variable.\n\nBut in any case, make sure you know the path to the `poetry` executable. The Poetry\ninstaller tells you where it has been installed, and how to configure your shell.\n\nPlease refer to the [Poetry docs](https://python-poetry.org/docs/) for guidance on\nusing Poetry.\n\n### Setup for PyCharm users\n\nYou can easily obtain the project files using PyCharm (menu "Git > Clone...").\nPyCharm will then usually prompt you to open the project.\n\nOpen the project in a new window. PyCharm will then usually prompt you to create\na new virtual environment.\n\nCreate a new Poetry virtual environment for the project. If PyCharm doesn\'t already\nknow where your `poetry` executable is, then set the path to your `poetry` executable\nin the "New Poetry Environment" form input field labelled "Poetry executable". In the\n"New Poetry Environment" form, you will also have the opportunity to select which\nPython executable will be used by the virtual environment.\n\nPyCharm will then create a new Poetry virtual environment for your project, using\na particular version of Python, and also install into this virtual environment the\nproject\'s package dependencies according to the project\'s `poetry.lock` file.\n\nYou can add different Poetry environments for different Python versions, and switch\nbetween them using the "Python Interpreter" settings of PyCharm. If you want to use\na version of Python that isn\'t installed, either use your favourite package manager,\nor install Python by downloading an installer for recent versions of Python directly\nfrom the [Python website](https://www.python.org/downloads/).\n\nOnce project dependencies have been installed, you should be able to run tests\nfrom within PyCharm (right-click on the `tests` folder and select the \'Run\' option).\n\nBecause of a conflict between pytest and PyCharm\'s debugger and the coverage tool,\nyou may need to add ``--no-cov`` as an option to the test runner template. Alternatively,\njust use the Python Standard Library\'s ``unittest`` module.\n\nYou should also be able to open a terminal window in PyCharm, and run the project\'s\nMakefile commands from the command line (see below).\n\n### Setup from command line\n\nObtain the project files, using Git or suitable alternative.\n\nIn a terminal application, change your current working directory\nto the root folder of the project files. There should be a Makefile\nin this folder.\n\nUse the Makefile to create a new Poetry virtual environment for the\nproject and install the project\'s package dependencies into it,\nusing the following command.\n\n    $ make install-packages\n\nIt\'s also possible to also install the project in \'editable mode\'.\n\n    $ make install\n\nPlease note, if you create the virtual environment in this way, and then try to\nopen the project in PyCharm and configure the project to use this virtual\nenvironment as an "Existing Poetry Environment", PyCharm sometimes has some\nissues (don\'t know why) which might be problematic. If you encounter such\nissues, you can resolve these issues by deleting the virtual environment\nand creating the Poetry virtual environment using PyCharm (see above).\n\n### Project Makefile commands\n\nYou can start EventStoreDB using the following command.\n\n    $ make start-eventstoredb\n\nYou can run tests using the following command (needs EventStoreDB to be running).\n\n    $ make test\n\nYou can stop EventStoreDB using the following command.\n\n    $ make stop-eventstoredb\n\nYou can check the formatting of the code using the following command.\n\n    $ make lint\n\nYou can reformat the code using the following command.\n\n    $ make fmt\n\nTests belong in `./tests`. Code-under-test belongs in `./esdbclient`.\n\nEdit package dependencies in `pyproject.toml`. Update installed packages (and the\n`poetry.lock` file) using the following command.\n\n    $ make update-packages\n',
    'author': 'John Bywater',
    'author_email': 'john.bywater@appropriatesoftware.net',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'https://github.com/pyeventsourcing/esdbclient',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.7,<4.0',
}


setup(**setup_kwargs)
